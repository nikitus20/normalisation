# Configuration for LLaMA-71M model

vocab_size: 32000
hidden_size: 512
intermediate_size: 2048
num_hidden_layers: 8
num_attention_heads: 8
max_position_embeddings: 2048
rms_norm_eps: 1e-6
attention_dropout: 0.0
hidden_dropout: 0.1